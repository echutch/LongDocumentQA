{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kpz36jcg2DfV"
      ],
      "mount_file_id": "1cyTgXMhS6eEt0xNwQGaqLIopfFG7NWp_",
      "authorship_tag": "ABX9TyNheVa1YBHnTs8gqyeCgYdO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/echutch/LongDocumentQA/blob/main/dbqa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document-Based Question Answering"
      ],
      "metadata": {
        "id": "pJ7QcdxJcOfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "Cl1x0eJ3cW0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community langchain-chroma pypdf chromadb hf_xet langchain-huggingface huggingface_hub \"langchain-google-genai>=0.0.6\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "cQ2EVd9_dtZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face Login"
      ],
      "metadata": {
        "id": "TBJYWXby3HcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "0e1PDNgX3GOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process Document"
      ],
      "metadata": {
        "id": "GP9t7xT4ox8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load in document (pdf)"
      ],
      "metadata": {
        "id": "L2H-zphjUhLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "def load(file):\n",
        "  loader = PyPDFLoader(file)\n",
        "  pages = loader.load()\n",
        "  return pages\n",
        "\n",
        "# TEST\n",
        "# pages = load('drive/MyDrive/dbqa/PaperQA.pdf')\n",
        "\n",
        "# print(f\"Number of pages: {len(pages)}\\n\")\n",
        "# print(f\"First 500 characters:\\n{pages[0].page_content[0:500]}\")\n",
        "# print(f\"\\nMeta Data: {pages[0].metadata}\")"
      ],
      "metadata": {
        "id": "RAm8dQ9hUL37",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split into chunks\n",
        "Play around with chunk sizes\n",
        "- Paper says size of 4000, but might be a little bit too big for prototype?"
      ],
      "metadata": {
        "id": "Kc4zCQS-Yqwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split(pages, chunk_size, chunk_overlap):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = chunk_size,\n",
        "      chunk_overlap = chunk_overlap\n",
        "  )\n",
        "\n",
        "  splits = text_splitter.split_documents(pages)\n",
        "  return splits\n",
        "\n",
        "# TEST\n",
        "# splits = split(pages, 1500, 150)\n",
        "# print(len(splits))\n",
        "# print(splits[1])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JIWkx2dZR2UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make embedding chunks, create vector database\n",
        "- Potentially try multiple models to find best result\n",
        "- Currently all-MiniLM-L6-v2"
      ],
      "metadata": {
        "id": "kZuHh5wBcH-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "def create_db(splits, persist_directory):\n",
        "  embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "  vectordb = Chroma.from_documents(\n",
        "      documents=splits,\n",
        "      embedding=embedding,\n",
        "      persist_directory=persist_directory\n",
        "  )\n",
        "  return vectordb"
      ],
      "metadata": {
        "id": "0v96hA6Xdm9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embed Document"
      ],
      "metadata": {
        "id": "_vnHWnNoJrI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'drive/MyDrive/dbqa/PaperQA.pdf'\n",
        "persist_directory = 'drive/MyDrive/dbqa/paperqa_db'"
      ],
      "metadata": {
        "id": "hZtjn7ytlJDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages = load(file_name)\n",
        "splits = split(pages, 1500, 150)\n",
        "vectordb = create_db(splits, persist_directory)"
      ],
      "metadata": {
        "id": "qUebBa0upV6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Propose Answer"
      ],
      "metadata": {
        "id": "1a4rfv4Vo8Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def answer(question, vectordb, llm):\n",
        "  template = \"\"\"Answer in a direct and concise tone, I am in a hurry. Your audience is an expert, so be\n",
        "  highly specific. If there are ambiguous terms or acronyms, first define them.\n",
        "  Write an answer with five sentences maximum for the question below based on the provided context.\n",
        "  If the context provides insufficient information, reply ''I cannot answer''. Answer in an unbiased, comprehensive,\n",
        "  and scholarly tone. If the question is subjective, provide an opinionated answer in the concluding 1-2 sentences.\n",
        "\n",
        "  {context}\n",
        "\n",
        "  Question: {question}\n",
        "\n",
        "  Answer:\"\"\"\n",
        "\n",
        "  QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "  retriever = vectordb.as_retriever(\n",
        "      search_type=\"mmr\",\n",
        "      search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
        "  )\n",
        "\n",
        "  # retriever = vectordb.as_retriever(\n",
        "  #       search_type=\"similarity\",\n",
        "  #       search_kwargs={\"k\": 10} # \"fetch_k\" is only used for mmr\n",
        "  #   )\n",
        "\n",
        "\n",
        "\n",
        "  qa_chain = RetrievalQA.from_chain_type(\n",
        "      llm=llm,\n",
        "      retriever=retriever,\n",
        "      return_source_documents=True,\n",
        "      chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        "  )\n",
        "\n",
        "  result = qa_chain.invoke({\"query\": question})\n",
        "\n",
        "   # Print the retrieved source documents\n",
        "  print(\"----- RETRIEVED CHUNKS -----\")\n",
        "  if result.get(\"source_documents\"):\n",
        "      for i, doc in enumerate(result[\"source_documents\"]):\n",
        "          # score = doc.metadata.get('distance', 'N/A')\n",
        "          # print(f\"Score: {score}\")\n",
        "          print(f\"Chunk {i + 1}:\")\n",
        "          print(doc.page_content)\n",
        "          print(\"------------------------------\")\n",
        "  return result\n",
        "\n",
        "def answer_long_context(question, pages, llm):\n",
        "  template = \"\"\"Answer in a direct and concise tone, I am in a hurry. Your audience is an expert, so be\n",
        "  highly specific. If there are ambiguous terms or acronyms, first define them.\n",
        "  Write an answer with five sentences maximum for the question below based on the provided context.\n",
        "  If the context provides insufficient information, reply ''I cannot answer''. Answer in an unbiased, comprehensive,\n",
        "  and scholarly tone. If the question is subjective, provide an opinionated answer in the concluding 1-2 sentences.\n",
        "\n",
        "  Context: {context}\n",
        "\n",
        "  Question: {question}\n",
        "\n",
        "  Answer:\"\"\"\n",
        "\n",
        "  QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "  chain = QA_CHAIN_PROMPT | llm\n",
        "\n",
        "  context = \"\\n\\n\".join([page.page_content for page in pages])\n",
        "\n",
        "  response = chain.invoke({\n",
        "        \"question\": question,\n",
        "        \"context\": context\n",
        "    })\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "saT1jKIo1VlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put it all together"
      ],
      "metadata": {
        "id": "mToUp-vypR4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "ukKGRmRsJuTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "persist_directory = 'drive/MyDrive/dbqa/paperqa_db'\n",
        "file_name = 'drive/MyDrive/dbqa/PaperQA.pdf'\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n",
        "pages = load(file_name)\n",
        "# question = \"Explain how the search tool works.\""
      ],
      "metadata": {
        "id": "dPsQ9O0PJxRn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Host on Colab"
      ],
      "metadata": {
        "id": "kpz36jcg2DfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from IPython.display import display\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=\"google/flan-t5-large\",\n",
        "        max_new_tokens=512,\n",
        "    )\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "result = answer(question, vectordb, llm)"
      ],
      "metadata": {
        "id": "LjMloj4-2HNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(result['result'])"
      ],
      "metadata": {
        "id": "-tPCRGlu3SrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gemini API (RAG)"
      ],
      "metadata": {
        "id": "rbVhUOjO3JDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=gemini_api_key)\n",
        "\n",
        "question = \"Highlight each tool of the PaperQA workflow and how they interact together.\"\n",
        "\n",
        "result = answer(question, vectordb, llm)\n",
        "display(result['result'])"
      ],
      "metadata": {
        "id": "6fvVeq9T3sig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Long Context"
      ],
      "metadata": {
        "id": "iYDwq0UtYLey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'drive/MyDrive/dbqa/IPCC_AR6_WGI_Chapter06.pdf'\n",
        "pages = load(file_name)"
      ],
      "metadata": {
        "id": "WB5pyPlmrzqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=gemini_api_key)\n",
        "\n",
        "question = \"How have Earth System Models (ESMs) evolved from the CMIP5 to the CMIP6 generation regarding aerosol processes?\"\n",
        "\n",
        "result = answer_long_context(question, pages, llm)\n",
        "display(result.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "lODDG2fhYP7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Successful Questions\n",
        "These are questions that the prototype answered correctly about the PaperQA paper."
      ],
      "metadata": {
        "id": "beNSGMNN_m93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"How does the PaperQA model work?\"\n",
        "- \"Explain how the search tool works.\"\n",
        "- \"How did the authors test the performance of the PaperQA model?\"\n",
        "- \"How much better did PaperQA perform compared to competing models?\" (pulled data from a table)\n",
        "- \"How did the researchers counteract hallucination?\"\n",
        "- \"What are some sources cited in this paper that I can learn more about retrieval augmented generation?\" (pulled several citations from the paper)\n",
        "- \"What would the cost per hour of PaperQA be?\"\n",
        "- \"What experiments were run on PaperQA?\n",
        "- \"What are some examples of questions that were in the LitQA dataset?\" (attempted to cite, correctly cited which table it was from but didn't know who the authors were)"
      ],
      "metadata": {
        "id": "pff4qk2KOrSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Failed Questions\n",
        "These are questions that the prototype was unable to answer. Some questions were supposed to fail (denoted) while others had the information contained within the paper."
      ],
      "metadata": {
        "id": "p63xNocD_005"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"Who are the authors of this paper?\"\n",
        "- \"What is the title of this paper?\"\n",
        "- \"How do I generate a new Google AI API key?\" (failed successfully)\n",
        "- \"If I wanted to build PaperQA myself, how could I do it?\" (I didn't expect this to work, but more reasoning based on the findings of the paper could be built in later)\n",
        "- \"What question did I ask you two questions ago?\""
      ],
      "metadata": {
        "id": "POYWN8X6OuUg"
      }
    }
  ]
}